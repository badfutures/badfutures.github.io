---
layout: post
title:  "1. Not believing one's eyes"
date:   2019-08-10 22:53 +1000
---

Very recently I listened to two world class pundits briefly discussing[^podcast] deepfakes and I was really surprised by the optimism about the impact and by their hope for a technical solution. While both gentlemen expressed concern[^concern], one of the pundits - whom I admire a great deal - went as far as to suggest that there may even be a positive aspect to the issue. 

My view is that proliferation of deepfakes changes everything and it is an existential threat to humanity. 

[^podcast]:  [Kevin Scott: Microsoft CTO \| Artificial Intelligence (AI) Podcast](https://lexfridman.com/kevin-scott/), published 2019 Aug 02

[^concern]: Kevin Scott says that he finds deepfakes worrisome (36:30) and says "we're not ready for this". 

As of August 2019 Wikipedia uses the following definition: "Deepfake (a portmanteau of "deep learning" and "fake") is a technique for human image synthesis based on artificial intelligence. It is used to combine and superimpose existing images and videos onto source images or videos using a machine learning technique known as generative adversarial network."[^deepfake_wiki] As of 2019 GANs (Generative Adversarial Networks), which are machine learning systems that use deep neural networks, are the cutting edge technology for creating deepfakes.

[^deepfake_wiki]: [Deepfake](https://en.wikipedia.org/wiki/Deepfake) in Wikipedia, retrieved 2019-08-11

It actually is not important if the fake clip is made using a deep neural network. The important issue is the appearance of being an unaltered representation of reality and for this post I will use the following definition:

> Deepfake is an image, video, audioclip, or other content that does not present real events but is distributed as if it was, and which to a human does not appear to be artificially generated or altered.

There were four theses postulated in the few minutes of the interview that was dedicated to deepfakes. I believe that all 4 were wrong.

**Fallacy 1. Broken trust = resilient individuals.** The claim here is that societies with no trust, with ever present propaganda, with an untrustworthy public discourse, were, in some sense, good. This is because this (un?)reality develops a kind of intellectual resiliency and encourages research. 

I challenge this view on multiple levels:
1. Numbers: Skepticism to good propaganda has been present in only a fraction of societies. The majority just believes. For a non political example see think of advertising.
1. Causation: It is not clear at all that the desire to question news is unleashed by living in a Soviet Union style reality. Generally, (un)inquisitive minds will be (un)inquisitive in all systems. 
1. Resiliency: One cannot avoid being fooled all the time. Each one of us believes in something that isn't true because we saw, heard, or read some material that wasn't true. 
1. Need: While skepticism can be seen as a trait worth having, it's a reaction to bad circumstances. The level of skepticism is correlated with how untrustworthy the society is - skepticism is an antibody for the cancer of broken trust. There is no intrinsic no need for skepticism. In my family I want trust, not skepticism. I want my children to trust me, not to expect lies. We should feel safe and we should not see dangers hiding in corners. 
1. Consequences: What doesn't kill you makes you stronger mantra doesn't generally make sense, including the specific case of common distrust. How often do you wish that you cannot trust whatyour friends and family and that from time to time they break your trust on purpose to build up your skepticism? How often do you hear advice to decit to make someone resilient and to encourage research?
   Let's consider these examples of consequences of lack of trust:
     1. A conspiracy theorist: I don't think a person who doesn't trust a whole branch of reality is a great role model or that their life feels good. 
     1. A victim of deception: A person who was a subject of deception can be affected for a long time, and prolonged deception can cause serious mental issues. Deception doesn't build, it destroys. 
         
**Fallacy 2. Silver lining.** The claim here is that "deepfakes (...) are creating global skepticism; (...) it encourages further research"[^lf-its-a-good-thing]

[^lf-its-a-good-thing]: Interview 40:45

This reasoning is similar to the first fallacy but applied to the future. Silver lining thinking has been used when discussing the political shifts of recent years but there is plenty of historical evidence that things need to turn really bad before societies start improving and that's only if they actually can start moving in the right direction without external forces. The most obvious example for my generation is [WWII](https://en.wikipedia.org/wiki/World_War_II). None of the stages of the deterioration of the state of democracy has shaken the world into action before it was too late. I believe that with deepfakes the damage will be too deep for the global society to repair itself and that there will be no external force to fix it. 

The unfounded hopefulness of both pundits is quite comical when it's expressed in the same breath with the need for skepticism. 

[//]: #This hopefulness by Lex, who is Jewish is really surprising. Remember Protocols of Zion or the posters of greedy Jews?

**Fallacy 3. Skepticism will save us.** The claim here is that skepticism is enough to deal with this. 

Firstly, I don't believe that there is that skepticism will appear. Why would it? We needed more skepticism for claims that vaccines don't cause autism, or that there are no [Elders of Zion](https://en.wikipedia.org/wiki/The_Protocols_of_the_Elders_of_Zion), but for many it never arrived.

Secondly, again, it's not possible to be skeptical all the time about everything.

**Fallacy 4. A technology will fix it** This claim here is that a technology will emerge that will address the problem. One example is signing the videos which will allow verification.

The signing idea immediately raises many questions. For example, how would this signing work when news is presented? We often consume clips embedded in other clips. I cannot see how signing could work for verifying if an embedded clip in a corner of another clip is authentic. 

Even if we only interact with complete signed clips, or the way we interact with clips changes, it's impractical to certify everything. We don't sign our emails and we don't confirm if the downloaded software wasn't tempered with. Even if my phone signs all photos automatically, what if it's stolen or accessed remotely? 

The main counterargument, however, is that it doesn't matter if we can technically verify if something is real. We just won't do it, even if it's easy. There is a lot of information out there that can be easily proved untrue, but some of these lies are widely believed. As Joscha Bach eloquently points out[^Joscha], we seek data to confirm our views rather than adjust our views to agree with the facts. 

[^Joscha]: Singularity FM, [Joscha Bach: We need to understand the nature of AI to understand who we are](https://www.singularityweblog.com/joscha-bach/), ~33:00 - 35:00, November 14, 2018	

It's important to point out that living in a society with broken trust is not the same as exposure to bad ideas. Iâ€™d argue that exposure is necessary for developing critical thinking and that we should not be using filtering to reduce it (filtering as in internet filter bubbles, sheltering children, or even removing bias from machine learning training data). I agree completely with Roman Yampolskiy when he says that we should teach bad ideas to students and show why they're bad[^yampolskiy18]. 

[^yampolskiy18]: Singularity FM, [Roman Yampolskiy on Artificial Intelligence Safety and Security](https://www.singularityweblog.com/tag/roman-yampolskiy/), ~1:08:00, August 31, 2018 

Deefakes are exponential tools for propaganda and the arrival of deepfakes is the match that will ignite gaslighing on global scale. "Gaslighting is a form of psychological manipulation in which a person seeks to sow seeds of doubt in a targeted individual or in members of a targeted group, making them question their own memory, perception, and sanity. Using persistent denial, misdirection, contradiction, and lying, gaslighting involves attempts to destabilize the victim and delegitimize the victim's belief.[^gaslighting]"

[^gaslighting]: [Gaslighting](https://en.wikipedia.org/wiki/Gaslighting) in Wikipedia

The most obviously dangerous deepfakes are the ones distributed for the purpose of gaslighting, but I believe that all deepfakes are extremely dangerous, even the ones seemingly beneficial (example from the interview: Microsoft uses deepfakes as input data for neural nets to avoid bias). This is because no matter what a deepfake is used for it breaks the chain of trust. It also creates a new history of the universe (as in multiple universes theory but opposite to the arrow of time). These different histories cannot be shared between individuals. 

Let's consider this mental experiment. Imagine that you agree with your friends and people to send one another deepfakes from time to time. You start getting deepfakes about the weather for the weekend, the time and place Friday gathering is at, and if a book is worth reading. How much of faking is needed before you stop trusting anything? How long would it take before you literally go crazy? 

I believe deepfakes will rip the fabric of society. I know that many things were described as dangerous to this ephemeral concept, but society *IS* interactions between people and if its members cannot trust any communication the outlook doesn't look good.

